import telebot
import requests
import json

TELEGRAM_TOKEN = "8563422388:AAGNMKKbmoR-JvgFxj6SNhVHW1HA80PFcjA"
OLLAMA_URL = "http://localhost:11434/api/chat"
bot = telebot.TeleBot(TELEGRAM_TOKEN)

user_context = {}

def safe_for_ai(text):
    blocked = ['Ø³ÙƒØ³', 'xxx', 'Ø¥Ø¨Ø§Ø­ÙŠØ§Øª', 'Ù†ÙŠÙƒ']
    return not any(word in text.lower() for word in blocked)

@bot.message_handler(commands=['/start'])
def start_message(message):
    user_context[message.chat.id] = {'custom_prompt': None}
    bot.reply_to(message, "ğŸ¤– **Ù†Ø§ØµÙØ­ AI** | Ø³Ø±Ø¹Ø© + Ø®ØµÙˆØµÙŠØ©\n\n`/prompt` `/reset` `/status`", parse_mode='Markdown')

@bot.message_handler(commands=['prompt'])
def set_prompt(message):
    chat_id = message.chat.id
    msg = bot.reply_to(message, "âœï¸ **Ø§Ù„Ù€ prompt Ø§Ù„Ø¬Ø¯ÙŠØ¯:**", parse_mode='Markdown')
    user_context[chat_id] = user_context.get(chat_id, {})
    user_context[chat_id]['waiting_prompt'] = True
    user_context[chat_id]['prompt_message_id'] = msg.message_id

@bot.message_handler(commands=['reset'])
def reset_context(message):
    chat_id = message.chat.id
    user_context[chat_id] = {'custom_prompt': None}
    bot.reply_to(message, "ğŸ”„ **Ø±ÙŠØ³ØªØ§Ø±Øª!** Ø¬Ø§Ù‡Ø² ğŸš€", parse_mode='Markdown')

@bot.message_handler(commands=['status'])
def show_status(message):
    chat_id = message.chat.id
    context = user_context.get(chat_id, {})
    status = "âœ… Ù…Ø®ØµØµ" if context.get('custom_prompt') else "ğŸ“‹ Ø§ÙØªØ±Ø§Ø¶ÙŠ"
    bot.reply_to(message, f"ğŸ“Š **Ø§Ù„Ø­Ø§Ù„Ø©:** {status}\nğŸ’¡ `/prompt`", parse_mode='Markdown')

@bot.message_handler(func=lambda m: True)
def handle_message(message):
    chat_id = message.chat.id
    text = message.text.strip()
    
    context = user_context.get(chat_id, {})
    if context.get('waiting_prompt'):
        user_context[chat_id]['custom_prompt'] = text
        user_context[chat_id]['waiting_prompt'] = False
        bot.edit_message_text(f"âœ… **ØªÙ… Ø§Ù„Ø­ÙØ¸!**\nğŸ“ *{text[:70]}...*", 
                            chat_id, context['prompt_message_id'], parse_mode='Markdown')
        return
    
    if not safe_for_ai(text):
        bot.reply_to(message, "ğŸ”’ **ØºÙŠØ± Ù…Ù†Ø§Ø³Ø¨**", parse_mode='Markdown')
        return
    
    loading_msg = bot.reply_to(message, "ğŸ§  **ÙŠØ­Ù„Ù„...** â³")
    
    # PROMPT Ù‚ÙˆÙŠ ÙŠÙØ±Ø¶ Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø¨Ø§Ù„Ø¶Ø¨Ø·
    base_prompt = context.get('custom_prompt') or """```
Ø£Ù†Øª Ù†Ø§ØµÙØ­ Ù…Ø§Ù„ÙŠ Ø³Ø¹ÙˆØ¯ÙŠ. Ø£Ø¬Ø¨ Ø¨Ù‡Ø°Ø§ Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø¨Ø§Ù„Ø¶Ø¨Ø·:

ğŸ§  **Ù†Ø§ØµÙØ­ | [Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹]**
âœ… **ÙÙ‡Ù…ØªÙƒ:** [ØªÙ„Ø®ÙŠØµ ÙˆØ§Ø­Ø¯]
ğŸ’° **Ø£Ù‚Ù„ Ø³Ø¹Ø±:** [Ø§Ù„Ù…Ø¨Ù„Øº + Ø§Ù„Ø¹Ù…Ù„Ø©]
ğŸ’¡ **Ø®Ø·Ø© (3 Ø®Ø·ÙˆØ§Øª):**
1ï¸âƒ£ [Ø®Ø·ÙˆØ© ÙˆØ§Ø¶Ø­Ø©]
2ï¸âƒ£ [Ø®Ø·ÙˆØ© ÙˆØ§Ø¶Ø­Ø©]
3ï¸âƒ£ [Ø®Ø·ÙˆØ© ÙˆØ§Ø¶Ø­Ø©]
â“ **Ø³Ø¤Ø§Ù„ÙŠ:** [Ø³Ø¤Ø§Ù„ ÙˆØ§Ø­Ø¯]

**Ø¥Ø¬Ø¨Ø§Ø±ÙŠ:**
- Ù„Ø§ ØªÙƒØªØ¨ Ø´ÙŠØ¡ Ù‚Ø¨Ù„ Ø£Ùˆ Ø¨Ø¹Ø¯ Ø§Ù„Ù‡ÙŠÙƒÙ„
- Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¥ÙŠÙ…ÙˆØ¬ÙŠ Ø§Ù„Ù…Ø­Ø¯Ø¯
- Ø±Ø¯ Ù‚ØµÙŠØ± Ø¬Ø¯Ø§Ù‹
- Ø£Ø±Ù‚Ø§Ù… Ø³Ø¹ÙˆØ¯ÙŠØ© 2026
```"""
    
    payload = {
        "model": "llama3.2:1b",
        "messages": [
            {"role": "system", "content": base_prompt},
            {"role": "user", "content": text}
        ],
        "stream": False,
        "options": {"temperature": 0.1, "num_predict": 300}
    }
    
    try:
        response = requests.post(OLLAMA_URL, json=payload, timeout=25)
        response.raise_for_status()
        
        ai_reply = response.json()['message']['content'].strip()
        final_reply = f"ğŸ¤– **Ù†Ø§ØµÙØ­ AI:**\n\n{ai_reply}"
        
        bot.edit_message_text(final_reply, chat_id, loading_msg.message_id, parse_mode='Markdown')
        
    except Exception:
        bot.edit_message_text("âŒ **Ø®Ø·Ø£:**\n-  ollama serve\n-  ollama pull llama3.2:1b", 
                            chat_id, loading_msg.message_id, parse_mode='Markdown')

if __name__ == "__main__":
    print("ğŸš€ Ù†Ø§ØµÙØ­ AI | Ø¬Ø§Ù‡Ø²!")
    bot.infinity_polling()
