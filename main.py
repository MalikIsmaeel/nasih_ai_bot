import telebot
import requests
import json
from datetime import datetime
import re

TELEGRAM_TOKEN = "8563422388:AAGNMKKbmoR-JvgFxj6SNhVHW1HA80PFcjA"
OLLAMA_URL = "http://localhost:11434/api/chat"
bot = telebot.TeleBot(TELEGRAM_TOKEN)

SENSITIVE_WORDS = ['Ø¬Ù†Ø³', 'Ø³ÙƒØ³', 'Ø¥Ø¨Ø§Ø­ÙŠ', 'xxx']
user_context = {}

@bot.message_handler(commands=['/start'])
def start_message(message):
    user_context[message.chat.id] = {'goal': None, 'custom_prompt': None}
    bot.reply_to(message, """
ğŸ¤– **Ù†Ø§ØµÙØ­ AI | Ø¨ÙˆØª Ø°ÙƒÙŠ Ù…Ø®ØµØµ**  

âœ… **Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª:**
â€¢ **Ø£Ù†Øª** ØªØªØ­ÙƒÙ… ÙÙŠ Ø§Ù„Ù€ Prompt  
â€¢ Ø³Ø±Ø¹Ø© 3-5 Ø«ÙˆØ§Ù†ÙŠ  
â€¢ Ø®ØµÙˆØµÙŠØ© 100% localhost  

**Ø§Ù„Ø£ÙˆØ§Ù…Ø±:**
`/prompt` - Ø¶Ø¨Ø· Ø§Ù„Ù€ prompt Ø§Ù„Ø®Ø§Øµ Ø¨ÙŠÙƒ  
`/reset` - Ø±ÙŠØ³ØªØ§Ø±Øª  
`/status` - Ø­Ø§Ù„Ø© Ø§Ù„Ø¨ÙˆØª  

Ø§Ø®ØªØ¨Ø±Ù†ÙŠ: `Ø¹Ø§ÙˆØ² Ø¬Ù‡Ø§Ø² Ø¹Ø±Ø³ Ø¨Ù€ 15 Ø£Ù„Ù`
    """, parse_mode='Markdown')

@bot.message_handler(commands=['prompt'])
def set_prompt(message):
    chat_id = message.chat.id
    msg = bot.reply_to(message, "âœï¸ **Ø§ÙƒØªØ¨ Ø§Ù„Ù€ prompt Ø§Ù„Ø¬Ø¯ÙŠØ¯:**\n\n*Ù…Ù„Ø§Ø­Ø¸Ø©: Ù‡ÙŠØ´ØªØºÙ„ Ù…Ø¹ Ø§Ù„Ø´Ø±ÙˆØ· Ø§Ù„Ø¥Ø¬Ø¨Ø§Ø±ÙŠØ©*", parse_mode='Markdown')
    
    user_context[chat_id]['waiting_prompt'] = True
    user_context[chat_id]['prompt_message_id'] = msg.message_id

@bot.message_handler(commands=['reset'])
def reset_context(message):
    chat_id = message.chat.id
    user_context[chat_id] = {'goal': None, 'custom_prompt': None}
    bot.reply_to(message, "ğŸ”„ **ØªÙ… Ø§Ù„Ø±ÙŠØ³ØªØ§Ø±Øª!** Ø¬Ø§Ù‡Ø² Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø¬Ø¯ÙŠØ¯Ø© ğŸš€")

@bot.message_handler(commands=['status'])
def show_status(message):
    chat_id = message.chat.id
    context = user_context.get(chat_id, {})
    prompt_status = "âœ… Ù…Ø®ØµØµ" if context.get('custom_prompt') else "ğŸ“‹ Ø§ÙØªØ±Ø§Ø¶ÙŠ"
    
    bot.reply_to(message, f"""
ğŸ“Š **Ø­Ø§Ù„Ø© Ø§Ù„Ø¨ÙˆØª:**
â€¢ Prompt: {prompt_status}
â€¢ Ù†Ù…ÙˆØ°Ø¬: llama3.2:1b

ğŸ’¡ ØºÙŠÙ‘Ø± Ø§Ù„Ù€ prompt Ø¨Ù€ `/prompt`
    """, parse_mode='Markdown')

@bot.message_handler(func=lambda message: True)
def handle_message(message):
    chat_id = message.chat.id
    text = message.text.strip()
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù†ØªØ¸Ø§Ø± Ø§Ù„Ù€ prompt
    if chat_id in user_context and user_context[chat_id].get('waiting_prompt'):
        custom_prompt = text
        user_context[chat_id]['custom_prompt'] = custom_prompt
        user_context[chat_id]['waiting_prompt'] = False
        
        bot.edit_message_text(
            f"âœ… **ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù€ Prompt Ø§Ù„Ø¬Ø¯ÙŠØ¯!**\n\nğŸ“ *{custom_prompt[:100]}...*\n\nØ¬Ø§Ù‡Ø² Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ğŸš€", 
            chat_id, 
            user_context[chat_id]['prompt_message_id'],
            parse_mode='Markdown'
        )
        return
    
    # ÙÙ„ØªØ±Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø­Ø³Ø§Ø³
    if any(word in text.lower() for word in SENSITIVE_WORDS):
        bot.reply_to(message, "ğŸ”’ **Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ ÙŠØ­ØªØ§Ø¬ Ù…ØªØ®ØµØµ Ù…Ø¹ØªÙ…Ø¯** ğŸ“\n\nğŸ’¡ Ø¬Ø±Ø¨: `/prompt` Ù„ØªØ®ØµÙŠØµ")
        return
    
    # Ø±Ø³Ø§Ù„Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„
    loading_msg = bot.reply_to(message, "ğŸ§  **Ù†Ø§ØµÙØ­ Ø¨ÙŠØ­Ù„Ù„...** â³")
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø³ÙŠØ§Ù‚
    if chat_id not in user_context:
        user_context[chat_id] = {}
    
    # Ø§Ù„Ù€ Prompt Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ (Ù…Ø®ØµØµ Ø£Ùˆ Ø§ÙØªØ±Ø§Ø¶ÙŠ)
    base_prompt = user_context[chat_id].get('custom_prompt')
    
    if not base_prompt:
        base_prompt = """
Ø£Ù†Øª Ù†Ø§ØµØ­ Ù…Ø§Ù„ÙŠ Ø³Ø¹ÙˆØ¯ÙŠ ğŸ¤. Ø±Ø¯ Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø¨Ù‡ÙŠÙƒÙ„ Ø«Ø§Ø¨Øª:

ğŸ§  **Ù†Ø§ØµÙØ­ | [Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹]**
âœ… **ÙÙ‡Ù…ØªÙƒ:** [ØªÙ„Ø®ÙŠØµ]
ğŸ’° **Ø£Ù‚Ù„ Ø³Ø¹Ø±:** [Ø§Ù„Ù…Ø¨Ù„Øº]
ğŸ’¡ **Ø®Ø·Ø© (3 Ø®Ø·ÙˆØ§Øª):**
1ï¸âƒ£ [Ø®Ø·ÙˆØ© 1]
2ï¸âƒ£ [Ø®Ø·ÙˆØ© 2] 
3ï¸âƒ£ [Ø®Ø·ÙˆØ© 3]
â“ **Ø³Ø¤Ø§Ù„ÙŠ:** [Ø³Ø¤Ø§Ù„ ÙˆØ§Ø­Ø¯]

**Ø´Ø±ÙˆØ· Ø¥Ø¬Ø¨Ø§Ø±ÙŠØ©:**
- Ø±Ø¯ Ù‚ØµÙŠØ± (6 Ø®Ø·ÙˆØ·)
- Ø£Ø±Ù‚Ø§Ù… + Ø¥ÙŠÙ…ÙˆØ¬ÙŠ ÙÙ‚Ø·
- Ø­Ù„ÙˆÙ„ Ø³Ø¹ÙˆØ¯ÙŠØ© ÙˆØ§Ù‚Ø¹ÙŠØ© 2026
        """
    
    # **Ø§Ù„ØªØºÙŠÙŠØ± Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ: Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ù…Ø¨Ø§Ø´Ø±Ø© Ø¨Ø¯ÙˆÙ† ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©**
    final_prompt = f"{base_prompt}\n\n**Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…:** {text}"
    
    # Ø·Ù„Ø¨ Ollama - Ø§Ù„Ø³Ø¤Ø§Ù„ Ù…Ø¨Ø§Ø´Ø±Ø©
    payload = {
        "model": "llama3.2:1b",
        "messages": [
            {"role": "system", "content": base_prompt},
            {"role": "user", "content": text}  # Ø§Ù„Ø³Ø¤Ø§Ù„ ÙÙ‚Ø·!
        ],
        "stream": False,
        "options": {
            "temperature": 0.3,
            "num_predict": 250,
            "top_p": 0.9
        }
    }
    
    try:
        response = requests.post(OLLAMA_URL, json=payload, timeout=30)
        response.raise_for_status()
        
        ai_reply = response.json()['message']['content'].strip()
        final_reply = f"ğŸ¤– **Ù†Ø§ØµÙØ­ AI:**\n\n{ai_reply}"
        
        bot.edit_message_text(
            final_reply, 
            chat_id, 
            loading_msg.message_id,
            parse_mode='Markdown'
        )
        
    except Exception as e:
        bot.edit_message_text(
            "âŒ **Ø®Ø·Ø£:**\nâ€¢ `ollama serve` Ø´ØºØ§Ù„ØŸ\nâ€¢ `ollama pull llama3.2:1b`ØŸ\n\n`/start` Ù„Ù„Ø±ÙŠØ³ØªØ§Ø±Øª", 
            chat_id, 
            loading_msg.message_id,
            parse_mode='Markdown'
        )

if __name__ == "__main__":
    print("ğŸš€ Ù†Ø§ØµÙØ­ AI Bot | Ø§Ù„Ø³Ø¤Ø§Ù„ Ù…Ø¨Ø§Ø´Ø±Ø© Ù„Ù„Ù€ AI!")
    print("âœ… Terminal 1: ollama serve")
    print("âœ… Terminal 2: python bot.py")
    bot.infinity_polling()
